# -*- coding: utf-8 -*-
"""
Created on Tue Feb 27 09:47:12 2024

@author: BISHWAJIT
"""

import matplotlib.pyplot as plt
import numpy as np

# Confusion matrix data for all classifiers
conf_matrices = {
    'Decision Tree': np.array([[399, 1, 5, 0, 1, 0, 0, 0],
                                [0, 110, 1, 0, 0, 12, 0, 0],
                                [5, 5, 1708, 0, 1, 3, 1, 1],
                                [0, 0, 0, 497, 0, 0, 0, 0],
                                [2, 0, 2, 0, 184, 0, 0, 2],
                                [2, 10, 0, 0, 3, 679, 0, 1],
                                [0, 0, 0, 0, 0, 0, 500, 0],
                                [1, 0, 4, 0, 0, 3, 0, 269]]),
    
    'Random Forest': np.array([[395, 2, 4, 0, 2, 3, 0, 0],
                                [0, 106, 3, 0, 1, 13, 0, 0],
                                [0, 0, 1724, 0, 0, 0, 0, 0],
                                [0, 0, 0, 497, 0, 0, 0, 0],
                                [1, 0, 6, 0, 181, 2, 0, 0],
                                [1, 8, 8, 0, 2, 675, 0, 1],
                                [2, 0, 0, 0, 0, 0, 498, 0],
                                [0, 0, 8, 0, 1, 4, 0, 264]]),
    
    'k-Nearest Neighbors': np.array([[350, 0, 12, 0, 0, 26, 16, 2],
                                      [2, 104, 4, 0, 1, 11, 0, 1],
                                      [10, 1, 1676, 0, 7, 21, 0, 9],
                                      [0, 0, 0, 497, 0, 0, 0, 0],
                                      [0, 1, 18, 0, 170, 1, 0, 0],
                                      [29, 12, 18, 0, 3, 626, 3, 4],
                                      [12, 0, 0, 0, 0, 3, 485, 0],
                                      [0, 0, 14, 0, 1, 5, 0, 257]]),
    
    'Naive Bayes': np.array([[140, 1, 2, 0, 5, 4, 254, 0],
                              [1, 117, 0, 0, 2, 1, 2, 0],
                              [179, 13, 755, 0, 610, 6, 150, 11],
                              [0, 0, 0, 497, 0, 0, 0, 0],
                              [2, 7, 0, 0, 179, 0, 1, 1],
                              [83, 40, 2, 0, 67, 64, 423, 16],
                              [0, 0, 0, 0, 0, 0, 500, 0],
                              [0, 6, 3, 0, 33, 5, 0, 230]]),
    
    'SVM Linear': np.array([[393, 3, 3, 1, 2, 4, 0, 0],
                             [3, 112, 2, 0, 2, 4, 0, 0],
                             [20, 7, 1677, 1, 7, 4, 1, 7],
                             [0, 0, 0, 497, 0, 0, 0, 0],
                             [4, 1, 7, 0, 177, 1, 0, 0],
                             [4, 10, 8, 1, 2, 665, 1, 4],
                             [0, 0, 0, 0, 0, 0, 500, 0],
                             [0, 2, 10, 0, 3, 3, 0, 259]]),
    
    'SVM Polynomial Degree 3': np.array([[0, 0, 406, 0, 0, 0, 0, 0],
                                          [0, 0, 123, 0, 0, 0, 0, 0],
                                          [0, 1, 1721, 0, 0, 1, 1, 0],
                                          [0, 0, 497, 0, 0, 0, 0, 0],
                                          [0, 0, 190, 0, 0, 0, 0, 0],
                                          [1, 1, 650, 0, 0, 42, 0, 1],
                                          [0, 0, 500, 0, 0, 0, 0, 0],
                                          [0, 0, 260, 0, 0, 6, 0, 11]]),
    
    'SVM Polynomial Degree 4': np.array([[0, 0, 406, 0, 0, 0, 0, 0],
                                          [0, 0, 123, 0, 0, 0, 0, 0],
                                          [1, 1, 1719, 0, 0, 1, 1, 1],
                                          [0, 0, 497, 0, 0, 0, 0, 0],
                                          [0, 0, 189, 0, 1, 0, 0, 0],
                                          [1, 1, 650, 0, 0, 42, 0, 1],
                                          [0, 0, 500, 0, 0, 0, 0, 0],
                                          [0, 0, 262, 0, 0, 4, 0, 11]]),
    
    'SVM RBF': np.array([[0, 0, 268, 3, 0, 2, 132, 1],
                          [0, 90, 26, 0, 0, 1, 6, 0],
                          [0, 9, 1697, 4, 0, 9, 1, 4],
                          [0, 0, 0, 497, 0, 0, 0, 0],
                          [0, 0, 166, 0, 17, 1, 6, 0],
                          [0, 8, 608, 1, 0, 72, 2, 4],
                          [0, 0, 129, 0, 0, 0, 371, 0],
                          [0, 0, 56, 3, 0, 7, 3, 208]]),
    
    'SVM Sigmoid': np.array([[0, 0, 401, 2, 0, 2, 0, 1],
                                         [0, 1, 122, 0, 0, 0, 0, 0],
                                         [0, 17, 1672, 3, 0, 26, 0, 6],
                                         [0, 0, 0, 497, 0, 0, 0, 0],
                                         [0, 0, 189, 0, 0, 1, 0, 0],
                                         [1, 7, 622, 1, 0, 55, 0, 9],
                                         [0, 0, 500, 0, 0, 0, 0, 0],
                                         [0, 0, 82, 10, 0, 13, 0, 172]]),
    'Xgboost': np.array([[401, 0, 3, 0, 1, 1, 0, 0],
                        [0, 113, 1, 0, 0, 9, 0, 0],
                        [0, 0, 1724, 0, 0, 0, 0, 0],
                        [0, 0, 0, 497, 0, 0, 0, 0],
                        [1, 1, 2, 0, 183, 2, 0, 1],
                        [0, 8, 1, 0, 3, 683, 0, 0],
                        [0, 0, 0, 0, 0, 0, 500, 0],
                        [0, 0, 4, 0, 0, 5, 0, 268]]),
    
    'Lightgbm': np.array([[402, 0, 2, 0, 1, 1, 0, 0],
                            [0, 110, 1, 0, 1, 11, 0, 0],
                            [0, 0, 1724, 0, 0, 0, 0, 0],
                            [0, 0, 0, 497, 0, 0, 0, 0],
                            [0, 0, 2, 0, 185, 2, 0, 1],
                            [1, 8, 0, 0, 2, 683, 0, 1],
                            [0, 0, 0, 0, 0, 0, 500, 0],
                            [0, 0, 4, 0, 1, 4, 0, 268]])
    
    
    
    
}

# Class names for the confusion matrix
class_names = ['Adware', 'Backdoor', 'Benign', 'Downloader', 'Spyware', 'Trojan', 'Virus', 'Worm']

# Plotting the confusion matrices for all classifiers
for clf_name, conf_matrix in conf_matrices.items():
    plt.figure(figsize=(10, 8))
    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix for {clf_name}')
    plt.colorbar()

    # Adding labels
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45)
    plt.yticks(tick_marks, class_names)

    # Adding values in the cells
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment='center', verticalalignment='center')

    plt.tight_layout()
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()
#%%                    BAR GRAPH
import matplotlib.pyplot as plt

# Classifier names
classifiers = [
    "Decision Tree",
    "Random Forest",
    "k-Nearest Neighbors",
    "Naive Bayes",
    "SVM Linear",
    "SVM Polynomial Degree 3",
    "SVM Polynomial Degree 4",
    "SVM RBF",
    "SVM Sigmoid",
    "XGBoost",
    "LightGBM"
]

# Evaluation metrics
accuracy = [
    0.985040797824116,
    0.9836808703535811,
    0.9440163191296465,
    0.5625566636446057,
    0.970081595648232,
    0.40208522212148684,
    0.40185856754306437,
    0.6690843155031732,
    0.5432910244786945,
    0.9902538531278332,  # XGBoost accuracy
    0.9902538531278332   # LightGBM accuracy
]

precision = [
    0.9715826547320184,
    0.9774752475392379,
    0.9316291456237995,
    0.6549439661765541,
    0.9477275637813007,
    0.27121434322521276,
    0.3919133740753768,
    0.7309935451375849,
    0.3695890766030701,
    0.9835010409543194,  # XGBoost precision
    0.9831409866928118   # LightGBM precision
]

recall = [
    0.9730381780989328,
    0.9634522768018662,
    0.9216261617405397,
    0.6998123296306552,
    0.95933984382089,
    0.13730033835010777,
    0.1378132214860218,
    0.5502524247201794,
    0.33475537338926253,
    0.9774730811266437,  # XGBoost recall
    0.9760479718859221   # LightGBM recall
]

f1_score = [
    0.9722848678330622,
    0.9702825932962881,
    0.9265152571817615,
    0.5769927200153593,
    0.953199104099012,
    0.09449818261007076,
    0.09574161407260148,
    0.5517345413551544,
    0.31323297959621277,
    0.9804452324431725,  # XGBoost F1 score
    0.9795204671781561   # LightGBM F1 score
]

# Plotting
x = range(len(classifiers))
width = 0.2

plt.figure(figsize=(12, 6))

plt.bar(x, accuracy, width, label='Accuracy')
plt.bar([i + width for i in x], precision, width, label='Precision')
plt.bar([i + width*2 for i in x], recall, width, label='Recall')
plt.bar([i + width*3 for i in x], f1_score, width, label='F1 Score')

plt.xlabel('Classifier')
plt.ylabel('Score')
plt.title('Evaluation Metrics by Classifier')
plt.xticks([i + width*1.5 for i in x], classifiers, rotation=45, ha='right')
plt.legend()

plt.tight_layout()
plt.show()
